{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Taxi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWw2Pkrjj6Z6"
      },
      "source": [
        "## There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrd1D0qKgoed"
      },
      "source": [
        "### Installing gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDzArN8RggJH",
        "outputId": "83310969-2828-4237-99b6-43f1b53735a5"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTR-E0B_hBBc"
      },
      "source": [
        "### Seeing how it looks like using render"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBjKuVNngmKm",
        "outputId": "8f0fe491-5892-4c3a-cba0-ffee1839f800"
      },
      "source": [
        "import gym\r\n",
        "env = gym.make(\"Taxi-v3\").env # We are using the .env on the end of make to avoid training stopping at 200 iterations\r\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RESbwOe0hJ4x",
        "outputId": "650907d9-8c2e-4709-bd36-cab72d57efc0"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\r\n",
        "env.render()\r\n",
        "print(\"Action Space {}\".format(env.action_space)) \r\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O5HPJS6iBe8"
      },
      "source": [
        "#### The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\r\n",
        "#### The pipe (\"|\") represents a wall which the taxi cannot cross.\r\n",
        "#### R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpTruSemiVEi"
      },
      "source": [
        "### As seen above we have an Action Space of size 6 and a State Space of size 500.\r\n",
        "#### Action number from 0-5 represents:\r\n",
        "  0 = south\r\n",
        "\r\n",
        "  1 = north\r\n",
        "  \r\n",
        "  2 = east\r\n",
        "  \r\n",
        "  3 = west\r\n",
        "  \r\n",
        "  4 = pickup\r\n",
        "  \r\n",
        "  5 = dropoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBlrtYPvi1gC"
      },
      "source": [
        "### We can encode the environment according to our wish too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eosSLWVLh347",
        "outputId": "de58705a-368b-43a0-826b-b93e7ee1b640"
      },
      "source": [
        "state = env.encode(2,1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\r\n",
        "print(\"State:\", state)\r\n",
        "env.s = state\r\n",
        "env.render()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 228\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGtaxHz5jEng",
        "outputId": "27c63f40-1e0f-42c8-b034-1e8ae40dae0c"
      },
      "source": [
        "env.P[228] # Reward table illustration"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 328, -1, False)],\n",
              " 1: [(1.0, 128, -1, False)],\n",
              " 2: [(1.0, 248, -1, False)],\n",
              " 3: [(1.0, 208, -1, False)],\n",
              " 4: [(1.0, 228, -10, False)],\n",
              " 5: [(1.0, 228, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQw28kIJjUkz"
      },
      "source": [
        "#### The dictionary above has the structure as : { action: [ ( probability, nextstate, reward, done) ] }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etEY8Xu6lIYe"
      },
      "source": [
        "### Initializing the Q-table to a 500Ã—6 matrix of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rBiPOI6jPpE"
      },
      "source": [
        "import numpy as np\r\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYbFswZwlUxb"
      },
      "source": [
        "### Training the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tCBzJqIlNUz",
        "outputId": "73804cc9-8da6-4b43-9db2-754af53ecfe8"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "import random\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "# Hyperparameters\r\n",
        "alpha = 0.1\r\n",
        "gamma = 0.6\r\n",
        "epsilon = 0.1\r\n",
        "\r\n",
        "# For plotting metrics\r\n",
        "all_epochs = []\r\n",
        "all_penalties = []\r\n",
        "\r\n",
        "for i in range(1, 100001):\r\n",
        "    state = env.reset()\r\n",
        "\r\n",
        "    epochs, penalties, reward, = 0, 0, 0\r\n",
        "    done = False\r\n",
        "    \r\n",
        "    while not done:\r\n",
        "        if random.uniform(0, 1) < epsilon:\r\n",
        "            action = env.action_space.sample() # Explore action space\r\n",
        "        else:\r\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\r\n",
        "\r\n",
        "        next_state, reward, done, info = env.step(action) \r\n",
        "        \r\n",
        "        old_value = q_table[state, action]\r\n",
        "        next_max = np.max(q_table[next_state])\r\n",
        "        \r\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\r\n",
        "        q_table[state, action] = new_value\r\n",
        "\r\n",
        "        if reward == -10:\r\n",
        "            penalties += 1\r\n",
        "\r\n",
        "        state = next_state\r\n",
        "        epochs += 1\r\n",
        "        \r\n",
        "    if i % 100 == 0:\r\n",
        "        clear_output(wait=True)\r\n",
        "        print(f\"Episode: {i}\")\r\n",
        "\r\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 59.1 s, sys: 9.51 s, total: 1min 8s\n",
            "Wall time: 1min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnrCgm3BlchW"
      },
      "source": [
        "#### Checking what the Q-values are at our illustration's state which we assumed above\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVvg9uH1lYN6",
        "outputId": "664d1781-770b-4d5b-ae07-76de1e326910"
      },
      "source": [
        "q_table[228]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.36395049,  -2.36395106,  -2.36395034,  -2.1220864 ,\n",
              "       -11.27324419, -11.27314499])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw7X4frZlwt_"
      },
      "source": [
        "The max Q-value is \"north\" (-2.27)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIh-EdiFl9M-"
      },
      "source": [
        "### Evaluate agent's performance after Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T36ztV6HlqWN",
        "outputId": "76cc4f90-c8e2-4e12-e13a-e0f9d5e92070"
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\r\n",
        "episodes = 100\r\n",
        "\r\n",
        "for _ in range(episodes):\r\n",
        "    state = env.reset()\r\n",
        "    epochs, penalties, reward = 0, 0, 0\r\n",
        "    \r\n",
        "    done = False\r\n",
        "    \r\n",
        "    while not done:\r\n",
        "        action = np.argmax(q_table[state])\r\n",
        "        state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "        if reward == -10:\r\n",
        "            penalties += 1\r\n",
        "\r\n",
        "        epochs += 1\r\n",
        "\r\n",
        "    total_penalties += penalties\r\n",
        "    total_epochs += epochs\r\n",
        "\r\n",
        "print(f\"Results after {episodes} episodes:\")\r\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\r\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.42\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_hBXPGmI7ZB"
      },
      "source": [
        "Notebook Link - https://colab.research.google.com/drive/1vmzDcji417m4-pfCIHc2MvoOnL7LKhkw?usp=sharing"
      ]
    }
  ]
}